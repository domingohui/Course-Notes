<!DOCTYPE html>
<html>
  <head>
    <a href="./index.html">Back to Courses</a>
    <title>MATH 146 Final Review by Raphael Koh</title>
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <link rel="stylesheet" href="./styles.css" />
  </head>
  <body>
    <h1 id="title_text">MATH 146 Notes by Raphael Koh</h1><br />
    <h1><a name="1"></a><b>1 </b>Vector Spaces & Dimensions</h1>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: A <b>field</b> is an algebraic system $\mathbb{F}$ having:
        <ul>
          <li>
            elements $0$, $1$, (and possibly more)
          </li>
          <li>
            operations $+, \times, -,$ and $^{-1}$
          </li>
        </ul>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: A <b>vector space over $\mathbb{F}$</b> is a set $V$ which is closed under addition and scalar multiplication, and satisfies VS 1-8.
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem 8 (Cancellation Law)</label>:</div>
        <div id="desc">
          Suppose $V$ is a vector space and $x,y,z\in V$.  If $x+z=y+z$ then $x=y$.
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Corollary 1</label>:</div>
        <div id="desc">
          In any vector space, there is only one zero vector, denoted $0$.
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Let $V$ be a vector space over $\mathbb{F}$.  A subset $W$ of $V$ is a <b>subspace of $V$</b> if $W$, with the operations of $V$, form a vector space.
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Let $V$ be a vector space over $\mathbb{F}$. Let $x, u_1, \ldots, u_n\in V$.  $x$ is a <b>linear combination</b> of $u_1,\ldots,u_n$ if there exists $a_1,\ldots,a_n\in\mathbb{F}$ such that $x=a_1u_1+a_2u_2+\ldots+a_nu_n$
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Suppose $V$ is a vector space over $\mathbb{F}$, $W\subseteq V$.  Then, $W$ is a subspace of $V$ if and only if:
          <ol>
            <li>
              $W$ is closed under the operations (addition + scalar multiplication) of $V$.
            </li>
            <li>
              $W$ contains the zero vector of $V$.
            </li>
          </ol>
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $V$ is a vs/$\mathbb{F}$, $x\in V$, and $\phi\neq S\subseteq V$.
        <ol>
          <li>
            $x$ is a <b>linear combination of $S$</b> if $x$ is a linear combination of some vectors $u_1,\ldots,u_n\in S$.
          </li>
          <li>
            $span (S)\overset{def}{=}\{\text{all linear combos of $S$}\}$<br />
            *$span(\phi)\overset{def}{=}\{0\}$
          </li>
        </ol>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Let $V$ be a vector space over $\mathbb{F}$, $S\subseteq V$.
          <ol>
            <li>
              $span(S)$ is a subspace of $V$
            </li>
            <li>
              $span(S)$ is the smallest subspace of $V$ containing $S$.
            </li>
          </ol>
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Let $V$ be a vs/$\mathbb{F}$, $x\in V$, and $S\subseteq V$.  $S$ is <b>linearly dependent</b> if there exist distinct vectors $u_1,\ldots,u_n\in S$ and $a_1,\ldots,a_n\in\mathbb{F}$ not all zero such that $a_1u_1+\ldots+a_nu_n=0$.
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: A set $S$ is linearly independent if $\forall$ distinct $u_1,\ldots,u_n\in S$, $\forall a_1,\ldots,a_n\in\mathbb{F}$, if $a_1u_1+\ldots+a_nu_n=0$ then $a_1=a_2=\ldots=a_n=0$.<br /><br />
        <i>Note:</i> $S=\phi$ is linearly independent because a linearly dependent set must be non-empty.  $S=\{0\}$ is linearly dependent because $a\cdot 0=0$ for any scalar $a$.
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Suppose $S$ is a linearly independent set in a vector space $V$ and $v\in V-S$.  <br />Then $S\cup\{v\}$ is linearly independent $\leftrightarrow v\not\in span(S)$.
        </div>
      </div>
    </div>
    <h2>Basis and Dimensions</h2>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: A subset of a vector space $V$ is a <b>basis</b> for $V$ if it is linearly independent and spans $V$.
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: A subset $S\subseteq V$ is a <b>minimal spanning set</b> if:
        <ol type="i">
          <li>
            $span(S)=V$ <u>and</u>
          </li>
          <li>
            no proper subset of $S$ is a spanning set of $V$.
          </li>
        </ol>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: A subset $S\subseteq V$ is a <b>maximal linearly independent</b> if:
        <ol type="i">
          <li>
            $S$ is linearly independent <u>and</u>
          </li>
          <li>
            No set $W\subseteq V$ that has $S$ as a proper subset is linearly independent.
          </li>
        </ol>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Basis $\longleftrightarrow$ Minimal spanning set $\longleftrightarrow$ Maximal linear independent set
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Replacement Lemma</label>:</div>
        <div id="desc">
          Suppose $V$ is a vector space and $S,T\subseteq V$, $v\in V$ and
          <ul>
            <li>
              $S$ us linearly independent.
            </li>
            <li>
              $v\not\in span(S)\rightarrow S\cup\{v\}$ is linearly independent
            </li>
            <li>
              $span(S\cup T)=V$
            </li>
          </ul>
          Then $\exists x\in T$ such that $span\bigg((S\cup\{v\})\cup(T-\{x\})\bigg)=V$
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Corollary</label>:</div>
        <div id="desc">
          If $V$ is finitely-generated, then all bases for $V$ are finite and have the same size.  This unique size of the basis of $V$ is known as the <b>dimension</b> of $V$, written as $\dim (V)$.
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          If $V$ is finitely-dimensional, then every linearly independent subset can be expanded to a basis for $V$.
        </div>
      </div>
    </div>
    <h1><a name="2"></a><b>2 </b>Functions Between Vector Spaces</h1>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $V$ and $W$ are vector spaces over $\mathbb{F}$.  A function $T: V\rightarrow W$ is a <b>linear transformation</b> if $T$ satisfies:
        <ol>
          <li>
            $T(x+y)=T(x)+T(y)$
          </li>
          <li>
            $T(ax)=aT(x)$
          </li>
        </ol>
      </div>
    </div>
    Properties of all linear transformations:
    <ol>
      <li>
        $T(0)=0$
      </li>
      <li>
        $T(-x)=-T(x)$
      </li>
      <li>
        $T(x-y)=T(x)-T(y)$
      </li>
      <li>
        $T(a_1x_1+\ldots+a_nx_n)=a_1T(x_1)+\ldots+a_nT(x_n)$
      </li>
    </ol>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $T: V\rightarrow W$ is linear, then
        <ul style="list-style: none;">
          <li>
            $R(T)$ is the <b>range</b> of $T$ such that $R(T)=\{T(x):x\in V\}$ and $R(T)$ is a subspace of $W$.
          </li>
          <li>
            $N(T)$ is the <b>nullspace</b> of $T$ such that $\{x\in V:T(x)=0\}$ and $N(T)$ is a subspace of $V$.
          </li>
        </ul>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Suppose $T: V\rightarrow W$ is linear and $V= span(v_1,\ldots, v_n)$ then $R(T)=span(T(v_1),\ldots,T(v_n))$.
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Suppose $T: V\rightarrow W$ is linear.
          <ul style="list-style:none;">
            <li>
              $T$ is <i>surjective</i> $\leftrightarrow R(T)=W$
            </li>
            <li>
              $T$ is <i>injective</i> $\leftrightarrow N(T)=\{0\}$
            </li>
          </ul>
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $T: V\rightarrow W$ is linear and $\dim (V)<\infty$.
        <ul style="list-style:none;">
          <li>
            $rank(T)=\dim(R(T))$
          </li>
          <li>
            $nullity(T)=\dim(N(T))$
          </li>
        </ul>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Rank-Nullity Theorem</label>:</div>
        <div id="desc">
          Suppose $T: V\rightarrow W$ is linear, then $rank(T)+nullity(T)=\dim(V)$.
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Corollary</label>:</div>
        <div id="desc">
          Suppose $T: V\rightarrow V$ and $\dim V<\infty$.  Then $T$ is injective $\leftrightarrow T$ is surjective.  In other words, $T$ is <i>bijective</i>.
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $V$ is a finite-dimensional vector space over $\mathbb{F}$ and $\beta=(v_1,\ldots, v_n)$ is an ordered basis for $V$, and $x\in V$.  The <b>coordinate vector of $x$ relative to $\beta$</b> is the $n$-tuple $(a_1,\ldots, a_n)\in\mathbb{F}^n$ unqiuely satisfying $x=a_1v_1+\ldots+a_nv_n$, and is denoted by
        $$[x]_\beta=\left(\begin{smallmatrix}a_1\\\vdots\\a_n\end{smallmatrix}\right)$$
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Suppose $V$ is a finite-dimensional vector space over $\mathbb{F}$ where $\dim(V)=n$ and $\beta=(v_1,\ldots,v_n)$ is an ordered basis. Then $[\ ]_\beta:V\rightarrow\mathbb{F}^n$ is a <u>bijective linear transformation</u> or an <u>isomorphism</u>.
        </div>
      </div>
    </div>
    Facts:
    <ol>
      <li>
        If $T:V\rightarrow W$ is an isomorphism, then $T^{-1}:W\rightarrow V$ is an isomorphism.
      </li>
      <li>
        If $T:V\rightarrow W$ and $U:W\rightarrow X$ are isomorphisms, then $U\circ T: V\rightarrow X$ is an isomorphism.
      </li>
    </ol>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $V,W$ are vs$/\mathbb{F}$.  $V$ is <u>isomorphic</u> to $W$ if $\exists$ isomorphism $T:V\rightarrow W$.
      </div>
    </div>
    <h3>Coordinating Linear Transformations</h3>
    Suppose $T:V\rightarrow W$ where $V,W$ are finite-dimensional.  Let $\beta=(v_1,\ldots,v_n),\gamma=(w_1,\ldots,w_m)$.  We observe that
    <ul>
      <li>
        $T$ is completely determined by $T(v_1),T(v_2),\ldots,T(v_n)$.
      </li>
      <li>
        Each $T(v_j)$ is a vector in $W$.
      </li>
      <li>
        Each $T(v_j)$ is characterized by its coordinate vector $[T(v_j)]_\gamma\in\mathbb{F}^n$.
      </li>
    </ul>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: In the above context, the <b>matrix representation of $T$</b> for $\beta$ and $\gamma$, denoted as $[T]_\beta^\gamma$, is the $mxn$ matrix in $M_{m\times n}(\mathbb{F})$ whose $j$th column is $[T(v_j)]_\gamma$.<br />
        So, $Col_j([T]_\beta^\gamma)=[T(v_j)]_\gamma$.
      </div>
    </div>
    Steps to form the matrix representation of $T$ for $\beta$ and $\gamma$:
    <ol>
      <li>
        Apply $T$ to each vector in $\beta$.
      </li>
      <li>
        Find coordinate vector of $T(v_j)$ relative to $\gamma$.
      </li>
      <li>
        Combine the column vectors to form the $m\times n$ matrix.
      </li>
    </ol>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Suppose $T:V\rightarrow W$ is linear and $V,W$ is a finite-dimensional vector space over $\mathbb{F}$ and $\beta=(v_1,\ldots,v_n)$, $\gamma=(w_1,\ldots,w_m)$ are ordered bases for $V,W$.  Then, for any $x\in V$,
          $$[T(x)]_\gamma=[T]_\beta^\gamma\cdot[x]_\beta$$
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          $$[U]_\beta^\gamma\cdot[T]_\alpha^\beta=[U\circ T]_\alpha^\gamma$$
        </div>
      </div>
    </div>
    Notation:
    <ul style="list-style:none;">
      <li>
         $\mathcal{L}(V,W)=\{$ all linear transformations $V\rightarrow W\ \}$
      </li>
      <li>
        $\mathcal{L}(V,V)=\mathcal{L}(V)$
      </li>
    </ul>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $\mathbb{F}$ is a field and $A\in M_{m\times n}(\mathbb{F})$.  We define $L_A: \mathbb{F}^n\rightarrow\mathbb{F}^m$ given by $L_A(x)=Ax$.
      </div>
    </div>
    Let $\beta_n$ be the standard ordered basis for $\mathbb{F}^n$, and let $\beta_m$ be the standard ordered basis for $\mathbb{F}^m$.  Note that $\forall x\in\mathbb{F}^m$, $[x]_{\beta_m}=x$.  To find $[L_A]_{\beta_n}^{\beta_m}$, we must find $L_A(e_1), \ldots,L_A(e_n)$.
    $$L_A(e_1)=Ae_1=\left(\begin{smallmatrix}a_{11}\\a_{21}\\\vdots\\a_{m1}\end{smallmatrix}\right)=Col_1(A)$$
    For all $j=1,\ldots, n$, $L_A(e_j)=Col_j(A)$. Since $[x]_{\beta_m}=x$, the $j$th column of $[L_A]_{\beta_n}^{\beta_m}=L_A(e_j)=Col_j(A)$.
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Corollary</label>:</div>
        <div id="desc">
          $R(L_A)=span(Col_1(A),\ldots,Col_n(A))$ or "range of $L_A=$ column space of $A$"
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: The <b>rank</b> of a matrix $A\in M_{m\times n}(\mathbb{F})$ is $rank(L_A)$ where $L_A:\mathbb{F}^n\rightarrow\mathbb{F}^m$ and $rank(L_A)=\dim(R(L_A))$.<br />
        Recall that $R(L_A))=$ subspace of $\mathbb{R}^m$ spanned by $Col(A)=$ column space of $A$.
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Lemma</label>:</div>
        <div id="desc">
          Multiplication by invertible matrices does not change rank.
        </div>
      </div>
    </div>
    In general, $R(g\circ f)=g\cdot R(f)$.  If $Q$ is invertible, $R(L_{AQ})=R(L_A)$, and $A$ and $AQ$ have the same column space.  $rank(A)=rank(AQ)$<br />
    However, left multiplication by an invertible matrix $P$ <u>can</u> change the column space (range of $L_A$) but <u>not</u> dimension.
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Lemma</label>:</div>
        <div id="desc">
          Isomorphisms preserve dimensions of subspaces.<br />
          Let $L_P$ be an isomorphism (invertible and bijective).  So,
          $$\dim(R(L_A))=\dim(L_P(R(L_A)))=\dim(R(L_{PA}))$$
          $$rank(A)=rank(PA)$$
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Suppose $A\in M_{m\times n}(\mathbb{F})$. $A$ is <u>invertible</u> if $\exists B\in M_{m\times n}(\mathbb{F})$ satisfying $AB=I_n$ amd $BA=I_n$.<br />
        If $A,B$ are invertible, then $AB$ is invertible and $(AB)^{-1}=B^{-1}A^{-1}$
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Let $Q=[I_V]_\beta^\gamma$, then
          <ol>
            <li>
              $Q$ is invertible.
            </li>
            <li>
              For any $x\in V$, $Q\cdot [x]_\beta=[I_V]_\beta^\gamma\cdot [x]_\gamma=[I_V(x)]_\gamma=[x]_\gamma$
            </li>
          </ol>
        </div>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: $Q=[I_V]_\beta^\gamma$ is called the <b>change of coordinate matrix from $\beta$ to $\gamma$</b>.
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          $$\begin{align*}
            [T]_\beta=Q^{-1}[T]_\gamma Q&=Q^{-1}[T]_\gamma [I_V]_\beta^\gamma\\
            &=[I_V]_\gamma^\beta[T]_\beta^\gamma\\
            &=[T]_\beta\quad\square
          \end{align*}$$
        </div>
      </div>
    </div>
    <h1><a name="3"></a><b>3 </b>Matrices and Systems of Equations</h1>
    <h3>Facts about Columns</h3>
    <ol>
      <li>
        $Col_j(AB)=A\cdot Col_j(B)$
      </li>
      <li>
        $Col_j(B)=B\cdot e_j$
      </li>
      <li>
        If $x=(x_1,\ldots,x_n)\in\mathbb{F}^n$, then $Bx=x_1Col_1(B)+x_2Col_2(B)+\ldots+x_nCol_n(B)$
      </li>
    </ol>
    <h3>Facts about Rows</h3>
    <ol>
      <li>
        $Row_i(AB)=Row_i(A)\cdot B$
      </li>
      <li>
        $Row_i(A)=e_i^tA\quad (e_i^t$ is a $1\times m$ vector)
      </li>
      <li>
        If $x=(x_1,\ldots,x_n)\in\mathbb{F}^n$, then $x^tA=x_1Row_1(A)+x_2Row_2(A)+\ldots+x_mRow_m(A)$
      </li>
    </ol>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Fix $m,n$.  An <b>elementary row operation</b> is one of the following actions. Given $A\in M_{m\times n}(\mathbb{F})$:
        <ol>
          <li>
            Switch row $i$ with row $j$.  $\quad[R_i\leftrightarrow R_j]$
          </li>
          <li>
            Mutltiply row $i$ with row $j$. $\quad[R_i\leftarrow aR_i\ (a\neq 0)]$
          </li>
          <li>
            Add to row $i$ a scalar multiple of row $j$. $\quad[R_i\leftarrow R_i + aR_j]$
          </li>
        </ol>
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="desc">
        <b><u>Definition</u></b>: Fix $m,n$.  An <b>elementary column operation</b> is defined in the same way as rows, but with columns.
      </div>
    </div>
    <div class="def" id="nocolour">
      <div id="title"><label>Definition</label>:</div>
      <div id="desc">
        An <b>elementary matrix</b> is an $m\times n$ matrix obtained from $I_n$ by <u>one</u> elementary operation.
        <ul style="list-style: none;">
          <li>
            Elementary column ops are simulated by elem matrices multiplying on the <u>right</u>.
          </li>
          <li>
            Elementary row ops are simulated by elem matrices multiplying on the <u>left</u>.
          </li>
        </ul>
      </div>
    </div>
    <h3>Facts about Elementary Matrices</h3>
    <ol>
      <li>
        Elementary Matrices are invertible.
      </li>
      <li>
        Elementary column ops ($A\mapsto AE$) do not change the column space.
      </li>
      <li>
        Elementary row ops ($A\mapsto EA$) can change the column space but not rank.
      </li>
    </ol>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          Every $A\in M_{m\times n}(\mathbb{F})$, there exists invertible matrices $P\in M_{n\times n}(\mathbb{F}), Q\in M_{m\times m}(\mathbb{F})$ such that $D=PAQ$ where
          $$D=\left(\begin{matrix}I_r & O\\O & O\end{matrix}\right)$$
          $rank(D)=\dim($column space of $D)=r=rank(A)$, so $rank(A)=r$.
        </div>
      </div>
    </div>
    <b><u>Transposes</u></b>: If $A$ is $m\times n$ and $B$ is $n\times p$, then $(AB)^t=B^tA^t$.
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Corollary</label>:</div>
        <div id="desc">
          If $A$ is $n\times n$ and is invertible, then $A^t$ is invertible.
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          For any $A\in M_{m\times n}(\mathbb{F}), rank(A)=rank(A^t)$.
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          The row space of $A$ is the subspace of $\mathbb{F}^n$ spanned by the rows of $A=$ column space of $A^t=R(L_{A^t})$.<br />
          But $\dim(L_{A^t})\overset{def}{=}rank(A^t)=rank(A)$, so $\dim(\text{row space of $A$})=\dim(\text{column space of $A$})$.
        </div>
      </div>
    </div>
    <div class="thm" id="impt">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          For any $A\in M_{n\times n}(\mathbb{F})$, then the following are equivalent:
          <ol>
            <li>
              $A$ is invertible.
            </li>
            <li>
              $rank(A)=n$
            </li>
            <li>
              $A$ is a product of elementary matrices.
            </li>
            <li>
              $A$ can be transformed to $I_n$ using elem row ops only.
            </li>
          </ol>
        </div>
      </div>
      <div id="Proof">
        <div id="title"><label>Proof</label>:</div>
        <div id="pf">
          <ul style="list-style: none;">
            <li style="display: flex;">
              <div style="padding-right: 5px;">
                $(1)\Leftrightarrow(2):$
              </div>
              <div>
                $A$ is invertible $\Longleftrightarrow L_A$ is a bijection $\Longleftrightarrow L_A$ is surjective $\Longleftrightarrow R(L_A)=\mathbb{F}^n\Longleftrightarrow \dim(R(L_A))=n\Longleftrightarrow rank(A)=n$
              </div>
            </li>
            <li style="display: flex;">
              <div style="padding-right: 5px;">
                $(2)\Rightarrow(3):$
              </div>
              <div>
                If $rank(A)=n$, then $A$ can be transformed by elementary matrices to $I_n$ where $I_n=E_k\cdots E_2E_1AE'_1E'_2\cdots E'_l=PAQ$.  Since $P,Q$ are invertible, we get
                $$\begin{align*}
                A&=P^{-1}(PAQ)Q^{-1}\\
                &=P^{-1}I_nQ^{-1}\\
                &=P^{-1}Q^{-1}\\
                &=(E_1)^{-1}(E_2)^{-1}\cdots(E_k)^{-1}(E'_l)^{-1}\cdots(E'_2)^{-1}(E'_1)^{-1}
                \end{align*}$$
                Since the inverse of an elementary matrix is also an elementary matrix, this proves (3).
              </div>
            </li>
            <li style="display: flex;">
              <div style="padding-right: 5px;">
                $(3)\Rightarrow(4):$
              </div>
              <div>
                Let $A=E_1E_2\cdots E_k$.  Then $A$ is invertible (product of elementary matrices which are invertible) and $A^{-1}=E^{-1}_1E^{-1}_2\cdots E^{-1}_k$. Thus, $I_n=A^{-1}A=E^{-1}_1E^{-1}_2\cdots E^{-1}_kA$.  Multiplying on the left by elementary matrices is the same as applying elem row operations.  Thus, this shows that $A$ can be transformed by elementary row operations to $I_n$.
              </div>
            </li>
            <li>
              $(4)\Rightarrow(2): rank(I_n)=n$ and elementary operations preserve rank, so $rank(A)=n$.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <div class="thm" id="nocolour">
      <div id="Theorem">
        <div id="title"><label>Theorem</label>:</div>
        <div id="desc">
          If $A$ can be transformed to $I_n$ by elem row ops, then the same sequence transforms $I_n$ to $A^{-1}$.  This is useful for calculating $A^{-1}$.
        </div>
      </div>
    </div>
    e.g. Let $A=\left(\begin{matrix} 1 & -3 &1\\1 & 0 &2\\ 0 &1 &0\end{matrix}\right)$.  Find $A^{-1}$.<br />
    Form the augmented matrix $(A|I_3)$.
    $$\left(\begin{array}{ccc} 1&-3&1\\1&0&2\\0&1&0 \end{array}\middle\vert\begin{array}{ccc} 1&0&0\\0&1&0\\0&0&1\end{array}\right)\xrightarrow{R_2\leftarrow R_2 - R_1}
      \left(\begin{array}{ccc} 1&-3&1\\0&3&1\\0&1&0 \end{array}\middle\vert\begin{array}{ccc} 1&0&0\\-1&1&0\\0&0&1\end{array}\right)\xrightarrow{R_1\leftarrow R_1 + R_2}
      \left(\begin{array}{ccc} 1&0&2\\0&3&1\\0&1&0 \end{array}\middle\vert\begin{array}{ccc} 0&1&0\\-1&1&0\\0&0&1\end{array}\right)$$
      $$\xrightarrow{R_2\leftrightarrow R_3}
      \left(\begin{array}{ccc} 1&0&2\\0&1&0\\0&3&1 \end{array}\middle\vert\begin{array}{ccc} 0&1&0\\0&0&1\\-1&1&0\end{array}\right)
      \xrightarrow{R_3\leftrightarrow R_3 - 3R_2}
      \left(\begin{array}{ccc} 1&0&2\\0&1&0\\0&0&1 \end{array}\middle\vert\begin{array}{ccc} 0&1&0\\0&0&1\\-1&1&-3\end{array}\right)
      \xrightarrow{R_1\leftrightarrow R_1 - 2R_3}
      \left(\begin{array}{ccc} 1&0&0\\0&1&0\\0&0&1 \end{array}\middle\vert\begin{array}{ccc} 2&-1&6\\0&0&1\\-1&1&-3\end{array}\right)$$
      $\therefore A^{-1}=\left(\begin{array}{ccc} 2&-1&6\\0&0&1\\-1&1&-3\end{array}\right)$<br />
      This algorithm also determines if $A$ is invertible.  If $A$ is not invertible, you will get $(I_r|B), r< n$ so $rank(A)\neq n\Leftrightarrow A$ is not invertible.
      <h2>Systems of Equations</h2>
      Consider a system, $S$, and $a_{ij},b_i\in\mathbb{F}$.
      $$\begin{align*}
        a_{11}x_1+a_{12}x_2+\ldots+a_{1n}x_n&=b_1\\
        a_{21}x_1+a_{22}x_2+\ldots+a_{2n}x_n&=b_2\\
        &\vdots\\
        a_{m1}x_1+a_{m2}x_2+\ldots+a_{mn}x_n&=b_m
      \end{align*}$$
      We can write $S$ as $\left(\begin{array}{cccc} a_{11}&a_{12}&\ldots&a_{1n}\\a_{21}&a_{22}&\ldots&a_{2n}\\&\vdots\\a_{m1}&a_{21}&\ldots&a_{mn}\end{array}\right)\left(\begin{matrix} x_1\\x_2\\\vdots\\x_n\end{matrix}\right)=\left(\begin{matrix} b_1\\b_2\\\vdots\\b_n\end{matrix}\right)$ which we write as $Ax=b$, where $A\in M_{m\times n}(\mathbb{F})$, $x$ is an unknown vector in $\mathbb{F}^n$, $b\in\mathbb{F}^m$.  <br /><br />
      A <u>solution</u> to $S$ is an $n$-tuple $s=(s_1,s_2,\ldots,s_n)\in\mathbb{F}^n$ satisfying $S$.<br />
      The <u>solution set</u> to $S$ is $\{s\in\mathbb{F}^n: As=b\}\subseteq\mathbb{F}^n$.
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: The system $Ax=0$ is called <b>homogenous</b>.<br />
          A system $Ax=b, b\neq 0$ is called <b>non-homogenous</b>.
        </div>
      </div>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Theorem</label>:</div>
          <div id="desc">
            Let $Ax=0$ be a homogenous system of $m$ linear equations in $n$ unknowns.
            <ol>
              <li>
                The solution set is $N(A)$.
              </li>
              <li>
                The solution set is a subspace of $\mathbb(F)^n$ of dimension: $n-rank(A)$.
              </li>
            </ol>
          </div>
        </div>
      </div>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Corollary</label>:</div>
          <div id="desc">
            If we have more unknowns than equations such that $n>m$ then $nullity(A)>0$ and the solution set has dimension $\geq 1$ so it has <u>non-zero</u> solutions.
          </div>
        </div>
      </div>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: A system $Ax=0$ is <b>consistent</b> if it has at least one solution.<br />
          <u>Note</u>: Every homogenous system $Ax=0$ is consistent (has the zero solution).
          <br /><br />
          $Ax=b$ is consistent $\Leftrightarrow rank(A)=rank(A|b)$.
        </div>
      </div>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Theorem</label>:</div>
          <div id="desc">
            Suppose $Ax=b$ is consistent.  Pick one solution $s\in\mathbb{F}^n$.  Then the solution set is $$s+N(A)\quad\text{(translation of $N(A)$ by $s$)}$$
          </div>
        </div>
      </div>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Theorem</label>:</div>
          <div id="desc">
            Suppose $Ax=b$ where $A$ is $m\times n$.  Then, $Ax=b\Leftrightarrow A$ is invertible.  Specifically, $s=A^{-1}b$.
          </div>
        </div>
      </div>
      <h2>Solving Systems of Linear Equations</h2>
      <u>3 core goals to solve $Ax=b$</u>:
      <ol>
        <li>
          Determine if $Ax=b$ is consistent.
        </li>
        <li>
          If yes, find one particular solution, $s_0$.
        </li>
        <li>
          Find a basis $\{u_1,\ldots,u_k\}$ for $N(A)$.
        </li>
      </ol>
      Then the solution to $Ax=b$ is $S=s_0+(a_1u_1+a_2u_2+\ldots+a_ku_k),$ where $a_1,\ldots,a_k\in\mathbb{F}$.<br /><br />
      e.g. Solve the following system of equations.
      $$\begin{align*}
        2x_1+3x_2+x_3+4x_4-9x_5&=17\\
        x_1+x_2+x_3+x_4-3x_5&=6\\
        x_1+x_2+x_3+2x_4-5x_5&=8\\
        2x_1+2x_2+2x_3+3x_4-8x_5&=14\\
      \end{align*}$$
      <ol>
        <li>
          Form the augmented matrix $(A|b)$.
          $$\left(\begin{array}{ccccc} 2&3&1&4&-9\\1&1&1&1&-3\\ 1&1&1&2&-5\\ 2&2&2&3&-8 \end{array}\middle\vert\begin{array}{c} 17\\6\\8\\14\end{array}\right)$$
        </li>
        <li>
          Reduce to <b>reduced row echelon form</b>.
          <div class="def" id="nocolour">
            <div id="desc">
              <b><u>Definition</u></b>: For $(A*|b*)$ to be in <b>reduced row echelon form (RREF)</b>:
              <ol>
                <li>
                  Zero row(s) must be at the bottom.
                </li>
                <li>
                  First (leftmost) non-zero entry in each row
                  <ol type="i">
                    <li>must equal $1$.</li>
                    <li>must be the only non-zero entry in its column</li>
                    <li>must be to the right of the first non-zero entry of the row above</li>
                  </ol>
                </li>
              </ol>
              <u>Fact</u>: Every matrix can be transformed by elem row ops to a RREF matrix.
            </div>
          </div>
          $$\left(\begin{array}{ccccc} 2&3&1&4&-9\\1&1&1&1&-3\\ 1&1&1&2&-5\\ 2&2&2&3&-8 \end{array}\middle\vert\begin{array}{c} 17\\6\\8\\14\end{array}\right)\xrightarrow{R_3\leftarrow R_3-R_2}
          \left(\begin{array}{ccccc} 2&3&1&4&-9\\1&1&1&1&-3\\ 0&0&0&1&-2\\ 2&2&2&3&-8 \end{array}\middle\vert\begin{array}{c} 17\\6\\2\\14\end{array}\right)\xrightarrow{R_1\leftarrow R_1-R_4}
          \left(\begin{array}{ccccc} 0&1&-1&1&-1\\1&1&1&1&-3\\ 0&0&0&1&-2\\ 2&2&2&3&-8 \end{array}\middle\vert\begin{array}{c} 3\\6\\2\\14\end{array}\right)$$
          $$\xrightarrow{R_4\leftarrow R_4-2R_2}
          \left(\begin{array}{ccccc} 0&1&-1&1&-1\\1&1&1&1&-3\\ 0&0&0&1&-2\\ 0&0&0&1&-2 \end{array}\middle\vert\begin{array}{c} 3\\6\\2\\2\end{array}\right)\xrightarrow{R_4\leftarrow R_4-R_3}
          \left(\begin{array}{ccccc} 0&1&-1&1&-1\\1&1&1&1&-3\\ 0&0&0&1&-2\\ 0&0&0&0&0 \end{array}\middle\vert\begin{array}{c} 3\\6\\2\\0\end{array}\right)\xrightarrow{R_2\leftarrow R_2-R_1}$$
          $$
          \left(\begin{array}{ccccc} 0&1&-1&1&-1\\1&0&2&0&-2\\ 0&0&0&1&-2\\ 0&0&0&0&0 \end{array}\middle\vert\begin{array}{c} 3\\3\\2\\0\end{array}\right)\xrightarrow{R_1\leftarrow R_1-R_3}
          \left(\begin{array}{ccccc} 0&1&-1&0&1\\1&0&2&0&-2\\ 0&0&0&1&-2\\ 0&0&0&0&0 \end{array}\middle\vert\begin{array}{c} 1\\3\\2\\0\end{array}\right)\xrightarrow{R_1\leftrightarrow R_2}
          \left(\begin{array}{ccccc} 1&0&2&0&-2\\0&1&-1&0&1\\ 0&0&0&1&-2\\ 0&0&0&0&0 \end{array}\middle\vert\begin{array}{c} 3\\1\\2\\0\end{array}\right)=(A^*|b^*)$$
        </li>
        <li>
          $rank(A)=rank(A^*)=rank(A^*|b^*)=rank(A|b)=3$, so $Ax=b$ is consistent.
        </li>
        <li>
          Solve $A^*x=b^*$ to find one particular solution.
          $$\begin{align*}
            x_1+2x_3-2x_5&=3\\
            x_2-x_3+x_5&=1\\
            x_4-2x_5&=2\\
          \end{align*}$$
          For a particular solution, let $x_3=x_5=0$.  Then $x_1=3,x_2=1,x_4=2$. So, $s_0=(3,1,0,2,0)$.
        </li>
        <li>
          Find a basis for $N(A)$.<br />
          $A$ is an $4\times 5$ matrix, so $L_A:\mathbb{F}^5\rightarrow\mathbb{F}^4$ so $\dim(\mathbb{F}^5)=5$.
          Since $rank(A)=3$, $nullity(A)=5-3=2$.<br />
          Recall that $N(A)$ is the solution to $Ax=0$, so
          $$\begin{align*}
            x_1&=-2x_3+2x_5\\
            x_2&=x_3-x_5\\
            x_3&=x_3\\
            x_4&=2x_5\\
            x_5&=x_5
          \end{align*}$$
          $$\left(\begin{matrix}x_1\\x_2\\x_3\\x_4\\x_5\end{matrix}\right)=x_3\left(\begin{matrix}-2\\1\\1\\0\\0\end{matrix}\right)+x_5\left(\begin{matrix}2\\-1\\0\\2\\1\end{matrix}\right)$$
          where $\{(-2,1,1,0,0),(2,-1,0,2,1)\}$ are basis vectors for $N(A)$.
        </li>
        <li>
          Form the general solution.
          $$s=s_0+a_1u_1+a_2u_2=(3,1,0,2,0)+a_1(-2,1,1,0,0)+a_2(2,-1,0,2,1)\quad a_1,a_2\in\mathbb{F}$$ <br />
          <i>This can be viewed as a translation of a plane in $\mathbb{F}^5$.</i>
        </li>
      </ol>
      Note that if $b=\left(\begin{smallmatrix}17\\6\\8\\15\end{smallmatrix}\right)$ instead, we would get $b^*=\left(\begin{smallmatrix}0\\0\\0\\1\end{smallmatrix}\right)$, so $rank(A^*|b^*)=4\neq rank(A)$, thus the system is inconsistent.
      <h1><a name="4"></a><b>4 </b>Determinants</h1>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: $$\det\left(\begin{smallmatrix}a &b\\c &d\end{smallmatrix}\right)=ad-bc$$
        </div>
      </div>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: Suppose $A$ is an $m\times n$ matrix and $1\leq i\leq m, 1\leq j\leq n$, then $\widetilde{A}_{ij}$ is the $(m-1)(n-1)$ matrix obtained from $A$ by deleting row $i$ and column $j$ from $A$.
        </div>
      </div>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: The <b>determinant</b> of a matrix is a mathematical object that is useful in the analysis and solution of systems of linear equations, and computing and establishing the properties of eigenvalues.  <br />
          Determinants are only defined for square matrices.
          <br /><br />
          Given $A\in M_{n\times n}(\mathbb{F}))$,
          <ol type="i">
            <li>
              If $n=1$, given $A=(a)$, then $\det(A)=a$.
            </li>
            <li>
              If $n > 1$, $$\det(A)=a_{11}\cdot\det(\widetilde{A}_{11})-a_{21}\cdot\det(\widetilde{A}_{21})+a_{31}\cdot\det(\widetilde{A}_{31})-\ldots+(-1)^{n+1}a_{n1}\cdot\det(\widetilde{A}_{n1})$$
              This is known as an <b>expansion of minors/cofactors on the first column</b>.<br /><br />
              $\det(\widetilde{A}_{i1})$ is the $(i,1)$-<b>minor</b> of $A$.<br />
              $(-1)^{i+1}\det(\widetilde{A}_{i1})$ is the $(i,1)$-<b>cofactor</b> of $A$.<br /><br />
              <u>General Form</u>:
              $$\det(A)=\sum_{i=1}^k(-1)^{i+j}a_{ij}\det(\widetilde{A}_{ij})$$
              which is the expansion on the $j$-th column.
            </li>
          </ol>
        </div>
      </div>
      <u>Properties</u>:
      Let $A\in M_{n\times n}$
      <ol>
        <li>
          If $A$ is upper-triangular, then $\det(A)=a_{11}a_{22}a_{33}\cdots a_{nn}$
        </li>
        <li>
          <u>Corollary</u>: $\det(I_n)=1$
        </li>
        <li>
          If $A$ has a row of zeros, then $\det(A)=0$
        </li>
        <li>
          If $A$ has a column of zeros, then $\det(A)=0$
        </li>
        <li>
          If $A$ has two equal adjacent rows, then $\det(A)=0$<br />
          <u>Corollary 9</u>: If $A$ has two equal rows, then $\det(A)=0$.
        </li>
        <li>
          $\det$ is <i>linear in each row</i>.  If we fix $n,i$, and $u_1,\ldots,u_{i-1},u_{i+1},\ldots,u_n\in\mathbb{F}^n$ then for all $r,s\in\mathbb{F}^n$ and all $a\in\mathbb{F}$,
          $$\det\left(\begin{array}{c}-u_1-\\\vdots\\-r+s-\\\vdots\\-u_n-\end{array}\right)=\det\left(\begin{array}{c}-u_1-\\\vdots\\-r-\\\vdots\\-u_n-\end{array}\right)+\det\left(\begin{array}{c}-u_1-\\\vdots\\-s-\\\vdots\\-u_n-\end{array}\right)$$
          $$\det\left(\begin{array}{c}-u_1-\\\vdots\\-ar-\\\vdots\\-u_n-\end{array}\right)=a\cdot\det\left(\begin{array}{c}-u_1-\\\vdots\\-r-\\\vdots\\-u_n-\end{array}\right)$$
          <i>Note that this does <b>not</b> mean that $\det(A+B)=\det(A)+\det(B)$ or $\det(aA)=a\det(A)$.</i>
        </li>
        <li>
          If $A\xrightarrow{R_i\leftarrow R_i+cR_j}B$ ($A$ is transformed by an elem row op of type 3) and $j=i\pm 1$, then $\det(A)=\det(B)$.  <br />
          i.e. Type 3 elem row ops preserve determinant when $j=i\pm 1$.<br />
          <u>Corollary 10</u>: <b>[Type 3]</b>  If $A\xrightarrow{R_i\leftarrow R_i+cR_j}B$, for $1\leq i,j\leq n$, then $\det(A)=\det(B)$.
        </li>
        <li>
          If $A\xrightarrow{R_i\leftrightarrow R_{i+1}}B$, then $\det(B)=-\det(A)$.<br />
          <u>Corollary 11</u>: <b>[Type 1]</b> If $A\xrightarrow{R_i\leftrightarrow R_j}B$, then $\det(B)=-\det(A)$.
        </li>
        <li>
          <u>Corollary 12</u>: <b>[Type 2]</b>  If $A\xrightarrow{R_i\leftrightarrow cR_i}B$, then $\det(B)=c\cdot\det(A)$.
        </li>
      </ol>
      We observe that given an elem matrix $E$, if $E$ is of Type:
      <ol>
        <li>$\det(E)=-\det(I_n)=-1$</li>
        <li>$\det(E)=c\cdot\det(I_n)=c,\quad c\neq 0$</li>
        <li>$\det(E)=\det(I_n)=1$</li>
      </ol>
      Note that $\det(E)\neq 0$ and that $\det(E^t)=\det(E)$.<br />
      <h3>How to Compute Determinants</h3>
      <ol>
        <li>
          We can use elem row ops to reduce $A$ to an upper triangular matrix.<br />
          e.g. Compute the determinant of $A=\left(\begin{array}{ccc} 0&1&3\\-2&-3&-5\\3&-1&1\end{array}\right)$
          $$\left(\begin{array}{ccc} 0&1&3\\-2&-3&-5\\3&-1&1\end{array}\right)\xrightarrow{R_1\leftrightarrow R_2}\left(\begin{array}{ccc} -2&-3&-5\\0&1&3\\3&-1&1\end{array}\right)\xrightarrow{R_3\leftrightarrow R_3+\frac{3}{2}R_1}\left(\begin{array}{ccc} -2&-3&-5\\0&1&3\\0&-\frac{11}{2}&-\frac{13}{2}\end{array}\right)\xrightarrow{R_3\leftrightarrow R_3+\frac{11}{2}R_1}\left(\begin{array}{ccc} -2&-3&-5\\0&1&3\\0&0&10\end{array}\right)$$
          $\det(B)=(-2)\cdot 1\cdot 10=-20$<br />
          $\det(A)=(-1)\cdot 1\cdot 1\cdot (-20) = 20$
        </li><br />
        <li>
          Transform $A$ by elem matrices to $I_n$.<br /><br />
          <b><u>Theorem 14</u></b>: If $A,E\in M_{n\times n}(\mathbb{F}), E$ is an elem matrix, then $\det(EA)=\det(E)\det(A)$.<br /><br />
          <b><u>Theorem 15</u></b>: $A$ is invertible $\Leftrightarrow \det(A)\neq 0$.
          <div class="thm" id="impt">
            <div id="Theorem">
              <div id="title"><label>Theorem 16</label>:</div>
              <div id="desc">
                $\det(AB)=\det(A)\det(B)$
              </div>
            </div>
            <div id="Proof">
              <div id="title"><label>Proof</label>:</div>
              <div id="pf">
                <ul style="list-style: none;">
                  <li>
                    <u>Case 1</u>: $rank(A)< n$. Then, $rank(AB)\leq rank(A)< n$ (by A6Q2b). Thus, $\det(A)=\det(AB)=0$.
                  </li>
                  <li>
                    <u>Case 2</u>: $rank(A)=n$.  Then $A$ is invertible and can be transformed into a product of elem matrices such that $A=E_k\cdots E_2E_1$.  Then,
                    $$\begin{align*}
                      \det(AB)&=\det(E_k\cdots E_2E_1B)\\
                      &=\det(E_k)\cdots \det(E_2)\det(E_1)\det(B)\\
                      &=\det(E_k\cdots E_2E_1)\det(B)\quad\text{(special case of Thm 14 where $A=I_n$)}\\
                      &=\det(A)\det(B)\quad\square
                    \end{align*}$$
                  </li>
                </ul>
              </div>
            </div>
          </div>
          e.g. Compute the determinant of $A$ as above.<br /><br />
          Continuing off from the above solution,
          $$\left(\begin{array}{ccc} -2&-3&-5\\0&1&3\\0&0&10\end{array}\right)\xrightarrow{R_1\leftrightarrow -\frac{1}{2}R_1}
          \left(\begin{array}{ccc} 1&\frac{3}{2}&\frac{5}{2}\\0&1&3\\0&0&10\end{array}\right)\xrightarrow{R_1\leftrightarrow R_1-\frac{3}{2}R_2}
          \left(\begin{array}{ccc} 1&0&-2\\0&1&3\\0&0&10\end{array}\right)\xrightarrow{R_1\leftrightarrow R_1-\frac{1}{5}R_3}
          \left(\begin{array}{ccc} 1&0&0\\0&1&3\\0&0&10\end{array}\right)$$
          $$\xrightarrow{R_2\leftrightarrow R_2-\frac{3}{10}R_3}
          \left(\begin{array}{ccc} 1&0&0\\0&1&0\\0&0&10\end{array}\right)\xrightarrow{R_3\leftrightarrow \frac{1}{10}R_3}
          \left(\begin{array}{ccc} 1&0&0\\0&1&0\\0&0&1\end{array}\right)$$
          $$\begin{align*}
            \det(I_n)&=\det(E_8E_7E_6E_5E_4E_3E_2E_1A)\\
            &=\det(E_8)\det(E_7)\det(E_6)\det(E_5)\det(E_4)\det(E_3)\det(E_2)\det(E_1)\det(A)\\
          \det(I_n)&=\frac{1}{10}\cdot -\frac{1}{2}\cdot -1\det(A)=\frac{1}{20}\det(A)\\
          \det(A)&=20\cdot\det(I_n)=20
          \end{align*}$$
        </li>
      </ol>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Corollary 17</label>:</div>
          <div id="desc">
            If $A$ is invertible then $\det(A^{-1})=\frac{1}{\det(A)}$.
          </div>
        </div>
      </div>
      <div class="thm" id="impt">
        <div id="Theorem">
          <div id="title"><label>Corollary 18</label>:</div>
          <div id="desc">
            $\det(A^t)=\det(A)$
          </div>
        </div>
        <div id="Proof">
          <div id="title"><label>Proof</label>:</div>
          <div id="pf">
            <ul style="list-style: none;">
              <li>
                <u>Case 1</u>: $rank(A)< n$. Then, $rank(A^t)=rank(A)< n$ (by A6Q2b). Thus, $\det(A^t)=\det(A)=0$.
              </li>
              <li>
                <u>Case 2</u>: $rank(A)=n$.  Then $A$ is invertible and can be transformed into a product of elem matrices such that $A=E_k\cdots E_2E_1$.  Then,
                $$\begin{align*}
                  A^t&=(E_k\cdots E_2E_1)^t=E_1^tE_2^t\cdots E_k^t\\
                  \det(A^t)&=\det(E_1^tE_2^t\cdots E_k^t)\\
                  &=\det(E_1^t)\det(E_2^t)\cdots \det(E_k^t)\\
                  &=\det(E_1)\det(E_2)\cdots \det(E_k)\\
                  &=\det(E_k)\cdots \det(E_2)\det(E_1)\\
                  &=\det(E_k\cdots E_2E_1)\\
                  &=\det(A)\quad\square
                \end{align*}$$
              </li>
            </ul>
          </div>
        </div>
      </div>
      By Corollary 18, anything we know about determinants involving rows can be used for columns as well.
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Lemma 20</label>:</div>
          <div id="desc">
            If $B$ is obtained from $A$ by <b>cyclically shifting</b> $k$ adjacent rows, then $\det(B)=(-1)^{k-1}\det(A)$.
          </div>
        </div>
      </div>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Lemma 21</label>:</div>
          <div id="desc">
            Suppose $A\in M_{n\times n}(\mathbb{F})$ whose $i$th row of $A$ is $e_j=(0,\ldots,0[i-1],1[i],0[i+1],\ldots,0)$. Then, $\det(A)=(-1)^{i+j}\det(\widetilde{A}_{ij})$.
          </div>
        </div>
      </div>
      <div class="thm" id="nocolour">
        <div id="Theorem">
          <div id="title"><label>Theorem 22</label>:</div>
          <div id="desc">
            $\det$ can be computed by expansion on minors on any row or column.  If $A\in M_{n\times n}(\mathbb{F})$, then
            <ul>
              <li>
                For any $i=1,\ldots, n$, then $\det(A)=\sum_{j=1}^n(-1)^{i+j}a_{ij}\cdot\det(\widetilde{A}_{ij})$.
              </li>
              <li>
                For any $j=1,\ldots, n$, then $\det(A)=\sum_{i=1}^n(-1)^{i+j}a_{ij}\cdot\det(\widetilde{A}_{ij})$.
              </li>
            </ul>
          </div>
        </div>
      </div>
      <h2>Permutations</h2>
      A <b>permutation</b> on $\{1,2,3,4\}$ is an <u>ordering</u>.  (e.g. $\{2,4,3,1\}$)<br />
      <u>Alternate View</u>: A bijection $\sigma : \{1,2,3,4\}\rightarrow\{2,4,3,1\}$, with $\sigma(1)=2,\sigma(2)=4,\sigma(3)=3,\sigma(4)=1$.<br />
      Also, $\sigma^{-1}(1)=4,\sigma^{-1}(2)=1,\sigma^{-1}(3)=3,\sigma^{-1}(4)=2$ <br />
      Representing permutations as such has two main advantages:
      <ol>
        <li>Can be graphed.</li>
        <li>
          Permutations can be composed.<br />
          Let $\tau: \{1,2,3,4\}\rightarrow\{3,2,1,4\}\quad$ <i>*$\tau$ is a <b>transposition</b></i><br />
          $\tau(1)=3,\tau(2)=2,\tau(3)=1,\tau(4)=4$.
          <ul style="list-style: none;">
            <li>$\sigma\circ\tau(1)=\sigma(3)=3$</li>
            <li>$\sigma\circ\tau(2)=\sigma(2)=4$</li>
            <li>$\sigma\circ\tau(3)=\sigma(1)=2$</li>
            <li>$\sigma\circ\tau(3)=\sigma(4)=1$</li>
          </ul>
        </li>
      </ol>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: A <b>transposition</b> is a permutation switching exactly two values.  Any permutation can be simulated by a sequence of row switches.
        </div>
      </div>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: Let $\sigma$ be a permutation on $\{1,\ldots,n\}$, $A,B\in M_{n\times n}(\mathbb{F})$, we write $A\xrightarrow{R:\sigma}B$ to denote $B$ obtained from $A$ by moving Row$_i(A)$ to row $\sigma(i)$.  So, Row$_i(B)=$Row$_{\sigma^{-1}(i)}(A)$.
        </div>
      </div>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: Let $\sigma$ be a permutation on $\{1,\ldots,n\}$, the <b>permutation matrix</b> associated to $\sigma$ is the matrix $P_\sigma$ obtained from $I_n$ by $\xrightarrow{R:\sigma}$, or, $I_n\xrightarrow{R:\sigma} P_\sigma$.
          $$P_\sigma=\left(\begin{array}{c}-e_{\sigma^{-1}(1)}-\\-e_{\sigma^{-1}(2)}-\\-e_{\sigma^{-1}(3)}-\\-e_{\sigma^{-1}(4)}-\end{array}\right)=
          \left(\begin{array}{c}-e_4-\\-e_1-\\-e_3-\\-e_2-\end{array}\right)=\left(\begin{array}{cccc}0&0&0&1\\1&0&0&0\\0&0&1&0\\0&1&0&0\end{array}\right)
          =\left(\begin{array}{cccc}|&|&|&|\\e_2&e_4&e_3&e_1\\|&|&|&|\end{array}\right)$$
          $$P_\sigma=\left(\begin{array}{cccc}|&|&|&|\\e_{\sigma(1)}&e_{\sigma(2)}&e_{\sigma(3)}&e_{\sigma(4)}\\|&|&|&|\end{array}\right)$$
        </div>
      </div>
      In general:
      <ul>
        <li>Row$_i(P_\sigma)=e_{\sigma^{-1}(i)}$</li>
        <li>Col$_i(P_\sigma)=e_{\sigma(i)}$</li>
        <li>$A\xrightarrow{R:\sigma} P_\sigma A$</li>
        <li>$P_\sigma e_i=e_{\sigma(i)}$</li>
        <li>$(P_\sigma P_\tau) e_i=P_\sigma (P_\tau e_i)=P_\sigma e_{\tau(i)}=e_{\sigma(\tau(i))}=e_{(\sigma\tau(i))}=(P_{\sigma\tau}e_i)\Rightarrow P_\sigma P_\tau=P_{\sigma\tau}$</li>
        <li>$P_{\sigma^{-1}}=P_\sigma^t$</li>
        <li>$\det(P_\tau)=-1$</li>
        <li>$\det(P_\sigma)=\det(P_{\tau_k}\cdots P_{\tau_2}P_{\tau_1})=\prod_{i=1}^k\det(P_{\tau_i})=(-1)^k=\pm 1$<br />
        <ul style="list-style:none;">
          <li>Even number of transpositions: sign$(\sigma)= \det(P_\sigma)=1 \Longrightarrow \sigma$ is <b>even</b></li>
          <li>Odd number of transpositions: sign$(\sigma)= \det(P_\sigma)=-1 \Longrightarrow \sigma$ is <b>odd</b></li>
        </ul>
        </li>
        <li>
          If $\sigma$ is composed of $k$ cycles of lengths $l_1,l_2,\ldots,l_k$, then sign$(\sigma)=(-1)^{(l_1-1)+(l_2-1)+\cdots+(l_k-1)}$
        </li>
      </ul>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: We can define the <b>complete expansion</b> of the determinant using permutations.  Given $A$ is an $n\times n$ matrix,
          $$\det(A)=\sum_\sigma\text{sign}(\sigma)a_{\sigma(1)1}a_{\sigma(2)2}\cdots a_{\sigma(n)n}$$
        </div>
      </div>
      <h1><a name="5"></a><b>5 </b>Eigenvalues</h1>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: Given a finite-dimensional vector space over $\mathbb{F}, V,$ a <b>linear operator</b> on $V$ is a linear transformation $T: V\rightarrow V$ denoted
          $$\mathcal{L}(V)=\{\text{all linear operators on }V\}$$
        </div>
      </div>
      <div class="def" id="nocolour">
        <div id="desc">
          <b><u>Definition</u></b>: Let $A\in M_{n\times n}(\mathbb{F})$.
          <ul style="list-style: none;">
            <li>An <b>eigenvector</b> of $A$ is any nonzero vector $v\in\mathbb{F}^n$ satisfying $Av\in span(v)$.</li>
            <li>The <b>eigenvalue</b> corresponding to $v$ is the unique scalar $\lambda\in\mathbb{F}$ such that $Av=\lambda v$.</li>
            <li>
              Given an eigenvalue $\lambda$ of $T\in\mathcal{L}(v)$, the <b>eigenspace</b> is defined as
              $$E_\lambda=\{v\in V: T(v)=\lambda b\}=\{\text{all eigenvectors of $T$ corresponding to $\lambda$}\}\cup\{0\}$$
            </li>
            <li>The <b>spectrum</b> of $T$ is the set of eigenvalues of $T$.</li>
          </ul>
        </div>
      </div>
  </body>
</html>
